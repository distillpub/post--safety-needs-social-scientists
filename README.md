AI safety needs social scientists
=================================

A call for social science researchers in long-term AI safety, to help understand how AI
alignment schemes work when actual humans are involved.  Where this paper lives:

1. [Distill paper](https://distill.pub/2019/safety-needs-social-scientists)
2. [Associated blog post](https://blog.openai.com/ai-safety-needs-social-scientists)

## Distill links

1. [Distill guide](https://distill.pub/guide) (unfortunately still using the old tag names).
2. [Example post](https://github.com/distillpub/post--example) from which this was cloned.

## Setup

How to set up for local editing:

    # Clone repo
    git clone https://github.com/distillpub/post--safety-needs-social-scientists

    # Install node dependencies
    cd post--safety-needs-social-scientists
    npm install

    # Run development server
    npm run dev

Then view at http://localhost:8080.
